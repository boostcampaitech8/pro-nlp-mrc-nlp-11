{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06b48d80",
   "metadata": {},
   "source": [
    "# Open-Domain QA: Train & Inference Walkthrough\n",
    "\n",
    "\n",
    "This notebook shows the minimal commands to prepare data, install dependencies, train DPR (optional but recommended for hybrid retrieval), train the MRC model, and run ODQA inference to produce `predictions.json`.\n",
    "\n",
    "\n",
    "> Run cells top-to-bottom. Adjust hyperparameters/paths as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f36622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environment (run this once at the start)\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"Using Python: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910017b4",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# Environment setup (install dependencies)\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3774a7",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Verify data layout (expects ../data with train/test + wikipedia_documents.json)\n",
    "!ls -l ../data || (cd .. && tar -xzf data.tar.gz && ls -l data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b30ad0",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "!python retrieval/DPR_train.py \\\n",
    "  --model_name_or_path klue/bert-base \\\n",
    "  --output_dir ./models/dpr \\\n",
    "  --device cuda \\\n",
    "  --learning_rate 2e-5 \\\n",
    "  --num_epochs 10 \\\n",
    "  --batch_size 32 \\\n",
    "  --gradient_accumulation_steps 2 \\\n",
    "  --max_q_length 64 \\\n",
    "  --max_p_length 256 \\\n",
    "  --use_hard_negatives True \\\n",
    "  --num_neg 2 \\\n",
    "  --warmup_steps 500 \\\n",
    "  --save_steps 500 \\\n",
    "  --eval_steps 500 \\\n",
    "  --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b32549d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73cdf2e",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Train MRC model (with retrieval-enabled eval)\n",
    "!python train.py \\\n",
    "  --output_dir ./models/train_dataset \\\n",
    "  --do_train --do_eval \\\n",
    "  --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cddca77",
   "metadata": {},
   "source": [
    "## Test Retrieval Performance\n",
    "\n",
    "Test and compare different retrieval methods (Sparse BM25, Dense DPR, Hybrid) on the validation set to see which performs best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326f15cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test retrieval performance on validation set\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "from retrieval.Sparse_retrieval import SparseRetrieval\n",
    "from retrieval.Dense_retrieval import DenseRetrieval\n",
    "from retrieval.retrieval import Retrieval\n",
    "import pandas as pd\n",
    "\n",
    "# Load validation dataset\n",
    "print(\"Loading validation dataset...\")\n",
    "datasets = load_from_disk(\"../data/train_dataset\")\n",
    "val_dataset = datasets[\"validation\"]\n",
    "print(f\"Validation set size: {len(val_dataset)}\")\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\", use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d08e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Sparse Retrieval (BM25) only\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Testing Sparse Retrieval (BM25)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "sparse_retriever = SparseRetrieval(\n",
    "    tokenize_fn=tokenizer.tokenize,\n",
    "    data_path=\"../data\",\n",
    "    context_path=\"wikipedia_documents.json\",\n",
    ")\n",
    "\n",
    "# Retrieve top-k passages for each question\n",
    "sparse_results = sparse_retriever.retrieve(val_dataset, topk=10)\n",
    "\n",
    "# Calculate accuracy (how many times the correct context was retrieved)\n",
    "if \"original_context\" in sparse_results.columns and \"context\" in sparse_results.columns:\n",
    "    # Check if original context is anywhere in the retrieved contexts\n",
    "    sparse_results[\"correct\"] = sparse_results.apply(\n",
    "        lambda row: row[\"original_context\"] in row[\"context\"], axis=1\n",
    "    )\n",
    "    sparse_accuracy = sparse_results[\"correct\"].sum() / len(sparse_results)\n",
    "    print(f\"Sparse Retrieval Accuracy: {sparse_accuracy:.2%}\")\n",
    "    print(f\"Correctly retrieved: {sparse_results['correct'].sum()}/{len(sparse_results)}\")\n",
    "else:\n",
    "    print(\"Cannot calculate accuracy - missing ground truth context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee52c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Dense Retrieval (DPR) only - requires trained encoders\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Testing Dense Retrieval (DPR)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    dense_retriever = DenseRetrieval(\n",
    "        model_name_or_path=\"klue/bert-base\",\n",
    "        data_path=\"../data\",\n",
    "        context_path=\"wikipedia_documents.json\",\n",
    "        q_encoder_path=\"./models/dpr/q_encoder\",\n",
    "        p_encoder_path=\"./models/dpr/p_encoder\",\n",
    "    )\n",
    "    \n",
    "    # Build dense embeddings\n",
    "    print(\"Building dense embeddings...\")\n",
    "    dense_retriever.get_dense_embedding()\n",
    "    print(f\"Dense embeddings shape: {dense_retriever.passage_embeddings.shape if hasattr(dense_retriever, 'passage_embeddings') else 'Not computed'}\")\n",
    "    \n",
    "    # Retrieve using dense method\n",
    "    print(\"Retrieving passages...\")\n",
    "    dense_results = dense_retriever.retrieve(val_dataset, topk=10)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    if \"original_context\" in dense_results.columns and \"context\" in dense_results.columns:\n",
    "        dense_results[\"correct\"] = dense_results.apply(\n",
    "            lambda row: row[\"original_context\"] in row[\"context\"], axis=1\n",
    "        )\n",
    "        dense_accuracy = dense_results[\"correct\"].sum() / len(dense_results)\n",
    "        print(f\"Dense Retrieval Accuracy: {dense_accuracy:.2%}\")\n",
    "        print(f\"Correctly retrieved: {dense_results['correct'].sum()}/{len(dense_results)}\")\n",
    "        \n",
    "        # Show sample results\n",
    "        print(\"\\nSample DPR Retrieval Results (first 3 examples):\")\n",
    "        for idx in range(min(3, len(dense_results))):\n",
    "            print(f\"\\nExample {idx + 1}:\")\n",
    "            print(f\"  Question: {dense_results.iloc[idx]['question'][:80]}...\")\n",
    "            print(f\"  Original context found: {dense_results.iloc[idx]['correct']}\")\n",
    "            print(f\"  Retrieved passage preview: {dense_results.iloc[idx]['context'][:100]}...\")\n",
    "    else:\n",
    "        print(\"Cannot calculate accuracy - missing ground truth context\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Dense retrieval failed: {e}\")\n",
    "    print(\"Make sure DPR encoders are trained (run DPR training cell first)\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b92a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Hybrid Retrieval (BM25 + DPR)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Testing Hybrid Retrieval (BM25 + DPR)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    hybrid_retriever = Retrieval(\n",
    "        tokenize_fn=tokenizer.tokenize,\n",
    "        data_path=\"../data\",\n",
    "        context_path=\"wikipedia_documents.json\",\n",
    "        use_sparse=True,\n",
    "        use_dense=True,\n",
    "        dense_model_path=\"klue/bert-base\",\n",
    "        q_encoder_path=\"./models/dpr/q_encoder\",\n",
    "        p_encoder_path=\"./models/dpr/p_encoder\",\n",
    "        sparse_weight=0.5,\n",
    "        dense_weight=0.5,\n",
    "    )\n",
    "    \n",
    "    # Retrieve using hybrid method\n",
    "    hybrid_results = hybrid_retriever.retrieve(val_dataset, topk=10)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    if \"original_context\" in hybrid_results.columns and \"context\" in hybrid_results.columns:\n",
    "        hybrid_results[\"correct\"] = hybrid_results.apply(\n",
    "            lambda row: row[\"original_context\"] in row[\"context\"], axis=1\n",
    "        )\n",
    "        hybrid_accuracy = hybrid_results[\"correct\"].sum() / len(hybrid_results)\n",
    "        print(f\"Hybrid Retrieval Accuracy: {hybrid_accuracy:.2%}\")\n",
    "        print(f\"Correctly retrieved: {hybrid_results['correct'].sum()}/{len(hybrid_results)}\")\n",
    "    else:\n",
    "        print(\"Cannot calculate accuracy - missing ground truth context\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Hybrid retrieval failed: {e}\")\n",
    "    print(\"Make sure DPR encoders are trained (run DPR training cell first)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08215e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RETRIEVAL PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "comparison_data = []\n",
    "\n",
    "# Add results if available\n",
    "if 'sparse_accuracy' in locals():\n",
    "    comparison_data.append({\n",
    "        'Method': 'Sparse (BM25)',\n",
    "        'Accuracy': f\"{sparse_accuracy:.2%}\",\n",
    "        'Correct': f\"{sparse_results['correct'].sum()}/{len(sparse_results)}\"\n",
    "    })\n",
    "\n",
    "if 'dense_accuracy' in locals():\n",
    "    comparison_data.append({\n",
    "        'Method': 'Dense (DPR)',\n",
    "        'Accuracy': f\"{dense_accuracy:.2%}\",\n",
    "        'Correct': f\"{dense_results['correct'].sum()}/{len(dense_results)}\"\n",
    "    })\n",
    "\n",
    "if 'hybrid_accuracy' in locals():\n",
    "    comparison_data.append({\n",
    "        'Method': 'Hybrid (BM25+DPR)',\n",
    "        'Accuracy': f\"{hybrid_accuracy:.2%}\",\n",
    "        'Correct': f\"{hybrid_results['correct'].sum()}/{len(hybrid_results)}\"\n",
    "    })\n",
    "\n",
    "if comparison_data:\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    print(\"\\nâœ“ Higher accuracy means the retrieval method finds the correct context more often\")\n",
    "else:\n",
    "    print(\"No results to compare. Run the test cells above first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb741a56",
   "metadata": {
    "vscode": {
     "languageId": "code"
    }
   },
   "outputs": [],
   "source": [
    "# Run ODQA inference on test set (produces predictions.json in output_dir)\n",
    "!python inference.py \\\n",
    "  --output_dir ./outputs/test_dataset \\\n",
    "  --dataset_name ../data/test_dataset \\\n",
    "  --model_name_or_path ./models/train_dataset \\\n",
    "  --do_predict \\\n",
    "  --eval_retrieval True \\\n",
    "  --top_k_retrieval 20 \\\n",
    "  --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f119ae65",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- Ensure GPU is available for DPR/MRC; set `--device cpu` if needed (slower).\n",
    "\n",
    "- `DPR_train.py` uses BM25 hard negatives; requires `rank_bm25` installed (in requirements).\n",
    "\n",
    "- If you change retrieval code, delete cached `sparse_embedding.bin` / `tfidfv.bin` and rerun.\n",
    "\n",
    "- `predictions.json` will be saved to `./outputs/test_dataset` in the inference step."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
